# -*- coding: utf-8 -*-
"""Task.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VZHws50USJUKae51ZSEveK78b3g5RHCy
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the data
file_path = '/content/adult.csv'
data = pd.read_csv(file_path)

# Display the first few rows of the dataset
print(data.head())

# Display basic information about the dataset
print(data.info())

# Check for missing values
missing_values = data.isnull().sum()
print("\nMissing values in each column:\n", missing_values)

# Check for duplicates
duplicates = data.duplicated().sum()
print("\nNumber of duplicate rows:", duplicates)

# Check for unique values in each column
unique_values = data.nunique()
print("\nUnique values in each column:\n", unique_values)

# Describe the dataset to check for outliers
print("\nSummary statistics of the dataset:\n", data.describe())

# Visualize distributions of numerical features to detect outliers
numerical_features = data.select_dtypes(include=[np.number]).columns
for feature in numerical_features:
    plt.figure(figsize=(10, 5))
    sns.boxplot(data[feature])
    plt.title(f'Distribution of {feature}')
    plt.show()

# Visualize distributions of numerical features to detect outliers
numerical_features = data.select_dtypes(include=[np.number]).columns
for feature in numerical_features:
    plt.figure(figsize=(10, 5))
    sns.boxplot(data[feature])
    plt.title(f'Distribution of {feature}')
    plt.show()
# Identify Outliers using IQR method
numerical_columns = data.select_dtypes(include=['number']).columns

for col in numerical_columns:
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    outliers = data[(data[col] < (Q1 - 1.5 * IQR)) | (data[col] > (Q3 + 1.5 * IQR))]
    print(f"Number of Outliers in {col}: {outliers.shape[0]}")

from sklearn.preprocessing import LabelEncoder, StandardScaler

# Remove duplicate rows
data = data.drop_duplicates()

# Label Encoding for categorical features
label_encoders = {}
categorical_features = data.select_dtypes(include=[object]).columns

for feature in categorical_features:
    le = LabelEncoder()
    data[feature] = le.fit_transform(data[feature])
    label_encoders[feature] = le

# Standard Scaling for numerical features
scaler = StandardScaler()
data[numerical_features] = scaler.fit_transform(data[numerical_features])

# Correlation matrix
correlation_matrix = data.corr()

# Plot the correlation matrix
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# distribute all classes and visualize

import matplotlib.pyplot as plt
# Assuming 'income' is the target variable for classification
target_variable = 'income'

# Count the occurrences of each class in the target variable
class_distribution = data[target_variable].value_counts()

# Visualize the class distribution
plt.figure(figsize=(8, 6))
sns.countplot(x=target_variable, data=data)
plt.title('Class Distribution of Target Variable')
plt.xlabel('Income Class')
plt.ylabel('Count')
plt.show()

# Print the class distribution
print(class_distribution)

from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE

# Encode categorical features
categorical_columns = ['workclass', 'education', 'marital-status', 'occupation',
                       'relationship', 'race', 'gender', 'native-country', 'income']

label_encoders = {}
for col in categorical_columns:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])
    label_encoders[col] = le
# Separate features and target variable
X = data.drop(['income', 'capital-gain'], axis=1)
y = data['income']

# Apply SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Check the new class distribution
pd.Series(y_resampled).value_counts()

#  data skewness

# Check skewness of numerical features
skewness = data.skew()
print("\nSkewness of numerical features:\n", skewness)

#  handle this skewness using log transformation when skew is positive  and square transformation when skew is negative and  make it and visualize distribution

import matplotlib.pyplot as plt
import numpy as np
# Handle skewness using transformations
for feature in numerical_features:
    if skewness[feature] > 0:
        data[feature] = np.log1p(data[feature])  # Log transformation for positive skew
    elif skewness[feature] < 0:
        data[feature] = data[feature] ** 2  # Square transformation for negative skew

# Visualize distributions after transformation
for feature in numerical_features:
    plt.figure(figsize=(10, 5))
    sns.histplot(data[feature], kde=True)
    plt.title(f'Distribution of {feature} after Transformation')
    plt.show()

# Import DecisionTreeClassifier
from sklearn.tree import DecisionTreeClassifier

# Split data into features (X) and target (y)
X = data.drop('income', axis=1)
y = data['income']

# Import train_test_split
from sklearn.model_selection import train_test_split
# Import GridSearchCV
from sklearn.model_selection import GridSearchCV
# Import accuracy_score
from sklearn.metrics import accuracy_score

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Decision Tree
# Hyperparameter tuning using GridSearchCV
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 5, 10, 15],
    'min_samples_split': [2, 5, 10]
}
grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5) # Now GridSearchCV is defined
grid_search.fit(X_train, y_train)

# Best Decision Tree model
best_dt = grid_search.best_estimator_

# Predictions and accuracy
y_pred_dt_train = best_dt.predict(X_train)
y_pred_dt_test = best_dt.predict(X_test)
accuracy_dt_train = accuracy_score(y_train, y_pred_dt_train) # Now accuracy_score is defined
accuracy_dt_test = accuracy_score(y_test, y_pred_dt_test)

print("Decision Tree - Train Accuracy:", accuracy_dt_train)
print("Decision Tree - Test Accuracy:", accuracy_dt_test)

# Visualize Decision Tree
# Assuming 'tree' is meant to be 'sklearn.tree'
from sklearn import tree
plt.figure(figsize=(15, 10))
tree.plot_tree(best_dt, feature_names=X.columns, class_names=['0', '1'], filled=True)
plt.show()

#  apply naive bayes

from sklearn.naive_bayes import GaussianNB
from imblearn.over_sampling import SMOTE # Import SMOTE
import pandas as pd # Import pandas for data manipulation
from sklearn.impute import SimpleImputer # Import SimpleImputer for handling missing values

# Assuming 'data' is your DataFrame containing the features
# Separate features and target variable (make sure this cell has been run before)
X = data.drop(['income', 'capital-gain'], axis=1)  # Define X here
y = data['income']

# Handle missing values using SimpleImputer
imputer = SimpleImputer(strategy='mean') # Replace missing values with the mean
X_imputed = imputer.fit_transform(X) # Fit and transform the features

# Initialize Gaussian Naive Bayes model
nb_model = GaussianNB()

# Use the imputed data for SMOTE
smote = SMOTE(random_state=42) # Initialize SMOTE
X_resampled, y_resampled = smote.fit_resample(X_imputed, y) # Apply SMOTE to imputed data

# Fit the model to the resampled data
nb_model.fit(X_resampled, y_resampled)

#  apply descion tree and random forest

# Import DecisionTreeClassifier
from sklearn.tree import DecisionTreeClassifier
# Import RandomForestClassifier
from sklearn.ensemble import RandomForestClassifier # Import the missing class

# Decision Tree
dt_classifier = DecisionTreeClassifier()
dt_classifier.fit(X_resampled, y_resampled)
# y_pred_dt = dt_classifier.predict(X_test)
# print("Accuracy (Decision Tree):", accuracy_score(y_test, y_pred_dt))
# print("Classification Report (Decision Tree):\n", classification_report(y_test, y_pred_dt))

# Random Forest
rf_classifier = RandomForestClassifier()
rf_classifier.fit(X_resampled, y_resampled)
# y_pred_rf = rf_classifier.predict(X_test)
# print("Accuracy (Random Forest):", accuracy_score(y_test, y_pred_rf))
# print("Classification Report (Random Forest):\n", classification_report(y_test, y_pred_rf))

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Split the resampled data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X_resampled, y_resampled, test_size=0.2, random_state=42
)

# Evaluate Naive Bayes
y_pred_nb = nb_model.predict(X_test)
print("Accuracy (Naive Bayes):", accuracy_score(y_test, y_pred_nb))
print("Classification Report (Naive Bayes):\n", classification_report(y_test, y_pred_nb))

# Evaluate Decision Tree
y_pred_dt = dt_classifier.predict(X_test)
print("Accuracy (Decision Tree):", accuracy_score(y_test, y_pred_dt))
print("Classification Report (Decision Tree):\n", classification_report(y_test, y_pred_dt))

# Evaluate Random Forest
y_pred_rf = rf_classifier.predict(X_test)
print("Accuracy (Random Forest):", accuracy_score(y_test, y_pred_rf))
print("Classification Report (Random Forest):\n", classification_report(y_test, y_pred_rf))

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Load the dataset
file_path = '/content/Mall_Customers.csv'
df = pd.read_csv(file_path)

# Data Exploration
print("Data Information:")
print(df.info())
print("\nFirst 5 Rows of the Data:")
print(df.head())

# Check for gender imbalance
gender_distribution = df['Gender'].value_counts()
print("\nGender Distribution:")
print(gender_distribution)

# Check for skewness in numerical columns
numerical_columns = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']
skewness = df[numerical_columns].skew()
print("\nSkewness in Numerical Columns:")
print(skewness)

# Function to identify outliers using the IQR method
def identify_outliers(data):
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = data[(data < lower_bound) | (data > upper_bound)]
    return outliers

# Identify outliers in each numerical column
outliers = {}
for column in numerical_columns:
    outliers[column] = identify_outliers(df[column])

# Print the count of outliers in each column
outliers_count = {col: len(outliers[col]) for col in outliers}
print("\nOutliers Count in Numerical Columns:")
print(outliers_count)

# Data Visualization
plt.figure(figsize=(12, 6))

# Age distribution
plt.subplot(1, 3, 1)
sns.histplot(df['Age'], kde=True, bins=20)
plt.title('Age Distribution')

# Annual Income distribution
plt.subplot(1, 3, 2)
sns.histplot(df['Annual Income (k$)'], kde=True, bins=20)
plt.title('Annual Income Distribution')

# Spending Score distribution
plt.subplot(1, 3, 3)
sns.histplot(df['Spending Score (1-100)'], kde=True, bins=20)
plt.title('Spending Score Distribution')

plt.tight_layout()
plt.show()

# K-means Clustering
# Standardize the data
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df[numerical_columns])

# Apply K-means clustering
kmeans = KMeans(n_clusters=5, random_state=42)
clusters = kmeans.fit_predict(df_scaled)

# Add the cluster labels to the original dataframe
df['Cluster'] = clusters

# Visualize the clusters
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x='Annual Income (k$)', y='Spending Score (1-100)', hue='Cluster', palette='viridis')
plt.title('K-means Clustering of Customers')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
# Install necessary libraries
!pip install kneed

from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors
from kneed import KneeLocator

# Determine optimal eps value using the elbow method
nearest_neighbors = NearestNeighbors(n_neighbors=11)
neighbors = nearest_neighbors.fit(df_scaled)
distances, indices = neighbors.kneighbors(df_scaled)
distances = np.sort(distances[:, 10], axis=0)
i = np.arange(len(distances))
knee = KneeLocator(i, distances, S=1, curve="convex", direction="increasing", interp_method='polynomial')
optimal_eps = distances[knee.knee]

# Apply DBSCAN clustering
dbscan = DBSCAN(eps=optimal_eps, min_samples=5)
clusters = dbscan.fit_predict(df_scaled)

# Add the cluster labels to the original dataframe
df['Cluster_DBSCAN'] = clusters

# Visualize the clusters
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x='Annual Income (k$)', y='Spending Score (1-100)', hue='Cluster_DBSCAN', palette='viridis')
plt.title('DBSCAN Clustering of Customers')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()

#import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.mixture import GaussianMixture

# Standardize the data (make sure this cell is executed before applying GMM)
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df[numerical_columns])  # Define df_scaled here

# Apply Gaussian Mixture Model
gmm = GaussianMixture(n_components=5, random_state=42)
clusters = gmm.fit_predict(df_scaled)


# Add the cluster labels to the original dataframe
df['Cluster_GMM'] = clusters

# Visualize the clusters
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x='Annual Income (k$)', y='Spending Score (1-100)', hue='Cluster_GMM', palette='viridis')
plt.title('Gaussian Mixture Model Clustering of Customers')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
from sklearn.cluster import AgglomerativeClustering

# Apply Hierarchical Clustering
hierarchical = AgglomerativeClustering(n_clusters=5)
clusters = hierarchical.fit_predict(df_scaled)

# Add the cluster labels to the original dataframe
df['Cluster_Hierarchical'] = clusters

# Visualize the clusters
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x='Annual Income (k$)', y='Spending Score (1-100)', hue='Cluster_Hierarchical', palette='viridis')
plt.title('Hierarchical Clustering of Customers')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()

import pandas as pd

# Load the dataset
df = pd.read_csv('/content/wine-clustering.csv')

# Inspect the dataset
print(df.head())
print(df.info())
print(df.describe())

# Check for missing values
print(df.isnull().sum())

# If there are missing values, handle them (e.g., fill with median or mean)
df.fillna(df.median(), inplace=True)

# Check the skewness of the features
print(df.skew())

# Apply transformation to reduce skewness
from sklearn.preprocessing import PowerTransformer

pt = PowerTransformer()
df_transformed = pt.fit_transform(df)

# Convert back to DataFrame
df_transformed = pd.DataFrame(df_transformed, columns=df.columns)

#  visualize the data

import matplotlib.pyplot as plt
import seaborn as sns

# Pairplot to visualize relationships between features
sns.pairplot(df_transformed)
plt.show()

# Correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(df_transformed.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

import numpy as np

# Define a function to remove outliers using Z-score
from scipy import stats

z_scores = np.abs(stats.zscore(df_transformed))
df_no_outliers = df_transformed[(z_scores < 3).all(axis=1)]

from sklearn.preprocessing import StandardScaler

# Standardize the data
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_no_outliers)

# Convert back to DataFrame
df_scaled = pd.DataFrame(df_scaled, columns=df_no_outliers.columns)

# Assuming you want to cluster based on 'Annual Income' and 'Spending Score'
X = df[['Alcohol', 'Malic_Acid']]

# Determine the optimal number of clusters using the Elbow method
inertia = []
for i in range(1, 11):
  kmeans = KMeans(n_clusters=i, random_state=42)
  kmeans.fit(X)
  inertia.append(kmeans.inertia_)

plt.plot(range(1, 11), inertia, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.show()

# Based on the Elbow method, choose the optimal number of clusters (e.g., 5)
kmeans = KMeans(n_clusters=5, random_state=42)
df['Cluster'] = kmeans.fit_predict(X)

# Visualize the clusters
plt.scatter(df['Alcohol'], df['Malic_Acid'], c=df['Cluster'])
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', label='Centroids')
plt.title('Customer Clusters')
plt.xlabel('Alcohol')
plt.ylabel('Malic_Acid')
plt.legend()
plt.show()

kmeans = KMeans(n_clusters=3, random_state=0)
kmeans.fit(df)
labels = kmeans.labels_

df['Cluster'] = labels

sns.scatterplot(x='Alcohol', y='Malic_Acid', hue='Cluster', data=df, palette='viridis')
plt.title('K-means Clustering')
plt.show()

from sklearn.cluster import DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan.fit(df)
labels = dbscan.labels_

df['Cluster'] = labels

sns.scatterplot(x='Alcohol', y='Malic_Acid', hue='Cluster', data=df, palette='viridis')
plt.title('DBSCAN Clustering')
plt.show()

from sklearn.mixture import GaussianMixture
gmm = GaussianMixture(n_components=3, random_state=0)
gmm.fit(df.drop('Cluster', axis=1))  # Fit GMM to features, excluding previous cluster labels
gmm_labels = gmm.predict(df.drop('Cluster', axis=1))
df['GMM_Cluster'] = gmm_labels

sns.scatterplot(x='Alcohol', y='Malic_Acid', hue='GMM_Cluster', data=df, palette='viridis')
plt.title('Gaussian Mixture Model Clustering')
plt.show()

from sklearn.cluster import AgglomerativeClustering

agg_clustering = AgglomerativeClustering(n_clusters=3)
agg_labels = agg_clustering.fit_predict(df.drop(['Cluster', 'GMM_Cluster'], axis=1))  # Exclude previous cluster labels
df['Agg_Cluster'] = agg_labels

sns.scatterplot(x='Alcohol', y='Malic_Acid', hue='Agg_Cluster', data=df, palette='viridis')
plt.title('Hierarchical Clustering')
plt.show()

import matplotlib.pyplot as plt
from sklearn.cluster import AgglomerativeClustering
import scipy.cluster.hierarchy as sch

# Assuming you want to cluster based on 'Annual Income' and 'Spending Score'
X = df[['Alcohol', 'Malic_Acid']]

# Create dendrogram
dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))
plt.title('Dendrogram')
plt.xlabel('Customers')
plt.ylabel('Euclidean Distances')
plt.show()

# Apply Agglomerative Clustering
hc = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')
df['Cluster'] = hc.fit_predict(X)

# Visualize the clusters
plt.scatter(df['Alcohol'], df['Malic_Acid'], c=df['Cluster'])
plt.title('Customer Clusters (Hierarchical)')
plt.xlabel('Alcohol')
plt.ylabel('Malic_Acid')
plt.show()

"""# **`INSURANCE CLAIMS\`**

months_as_customer: Number of months the customer has been with the insurance company.
age: Age of the insured individual.
policy_number: Unique identifier for the insurance policy.
policy_bind_date: Date when the insurance policy was bound or activated.
policy_state: State in which the insurance policy was issued.
policy_csl: Combined single limit of the policy (liability coverage).
policy_deductable: Deductible amount for the insurance policy.
policy_annual_premium: Annual premium amount for the insurance policy.
umbrella_limit: Limit of umbrella insurance coverage.
insured_zip: ZIP code of the insured individual.
insured_sex: Gender of the insured individual.
insured_education_level: Education level of the insured individual.
insured_occupation: Occupation of the insured individual.
insured_hobbies: Hobbies of the insured individual.
insured_relationship: Relationship status of the insured individual (e.g., single, married).
capital-gains: Capital gains reported by the insured individual.
capital-loss: Capital losses reported by the insured individual.
incident_date: Date when the incident occurred.
incident_type: Type of incident (e.g., collision, theft).
collision_type: Specific type of collision (if applicable).
incident_severity: Severity of the incident (e.g., minor, major).
authorities_contacted: Whether authorities were contacted (e.g., police, fire department).
incident_state: State where the incident occurred.
incident_city: City where the incident occurred.
incident_location: Specific location of the incident.
incident_hour_of_the_day: Hour of the day when the incident occurred.
number_of_vehicles_involved: Number of vehicles involved in the incident.
property_damage: Whether property damage occurred.
bodily_injuries: Number of bodily injuries reported.
witnesses: Number of witnesses to the incident.
police_report_available: Whether a police report is available.
total_claim_amount: Total amount claimed.
injury_claim: Amount claimed for injuries.
property_claim: Amount claimed for property damage.
vehicle_claim: Amount claimed for vehicle damage.
auto_make: Make of the insured vehicle.
auto_model: Model of the insured vehicle.
auto_year: Year of manufacture of the insured vehicle.
fraud_reported: Whether fraud was reported (yes or no).
_c39: Additional column that might be an error or extraneous.
"""

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('/content/insurance_claims.csv')

# Assuming 'df' is your DataFrame
for col in df.select_dtypes(include='object').columns:
  df[col] = df[col].apply(lambda x: re.sub(r'[^a-zA-Z0-9\s]', '', str(x)))

# Drop unnecessary columns (e.g., policy_number, _c39)
columns_to_drop = ['policy_number','policy_state', 'policy_bind_date', 'incident_date', 'incident_location',
                   'insured_zip','incident_city','insured_hobbies','auto_make','auto_model','auto_year', 'insured_education_level', '_c39']
df.drop(columns=columns_to_drop, inplace=True)
# Handle missing values
df.fillna(method='ffill', inplace=True)

df.drop_duplicates(inplace=True)

# Encode target variable
df['fraud_reported'] = df['fraud_reported'].apply(lambda x: 1 if x == 'Y' else 0)

# Separate features and target variable
X = df.drop(columns=['fraud_reported'])
y = df['fraud_reported']

# visualize features

import matplotlib.pyplot as plt
import seaborn as sns

# Visualize numerical features
numerical_features = X.select_dtypes(include=['int64', 'float64']).columns
for feature in numerical_features:
  plt.figure(figsize=(8, 6))
  sns.histplot(X[feature], kde=True)
  plt.title(f'Distribution of {feature}')
  plt.xlabel(feature)
  plt.ylabel('Frequency')
  plt.show()

# Visualize categorical features
categorical_features = X.select_dtypes(include=['object']).columns
for feature in categorical_features:
  plt.figure(figsize=(8, 6))
  sns.countplot(x=feature, data=X)
  plt.title(f'Distribution of {feature}')
  plt.xlabel(feature)
  plt.ylabel('Count')
  plt.xticks(rotation=45)
  plt.show()

# visualize outlires

import matplotlib.pyplot as plt
# Visualize outliers using box plots
numerical_features = X.select_dtypes(include=['int64', 'float64']).columns
for feature in numerical_features:
  plt.figure(figsize=(8, 6))
  sns.boxplot(x=X[feature])
  plt.title(f'Box Plot of {feature}')
  plt.xlabel(feature)
  plt.show()

# visualize data skewness

import matplotlib.pyplot as plt
# Visualize skewness using histograms
numerical_features = X.select_dtypes(include=['int64', 'float64']).columns
for feature in numerical_features:
  plt.figure(figsize=(8, 6))
  sns.histplot(X[feature], kde=True)
  plt.title(f'Distribution of {feature}')
  plt.xlabel(feature)
  plt.ylabel('Frequency')
  plt.show()

# Identify numerical features
numerical_features = X.select_dtypes(include=['int64', 'float64']).columns

# Handle outliers using IQR method
def handle_outliers(df, columns):
    for col in columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])
        df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])
    return df

X = handle_outliers(X, numerical_features)

# Handle skewness using log transformation
def handle_skewness(df, columns):
    for col in columns:
        if df[col].skew() > 1:
            df[col] = np.log1p(df[col])
        elif df[col].skew() < 0:
          df[col] = df[col] ** 2
    return df

X = handle_skewness(X, numerical_features)

# visualize  data skewness after handling

import matplotlib.pyplot as plt
# Visualize skewness using histograms after handling
numerical_features = X.select_dtypes(include=['int64', 'float64']).columns
for feature in numerical_features:
  plt.figure(figsize=(8, 6))
  sns.histplot(X[feature], kde=True)
  plt.title(f'Distribution of {feature} (After Skewness Handling)')
  plt.xlabel(feature)
  plt.ylabel('Frequency')
  plt.show()

# visualize data imbalance

import matplotlib.pyplot as plt
# Visualize class imbalance
plt.figure(figsize=(6, 4))
sns.countplot(x='fraud_reported', data=df)
plt.title('Class Distribution of Fraud Reported')
plt.xlabel('Fraud Reported (0: No, 1: Yes)')
plt.ylabel('Count')
plt.show()

# Identify categorical features
categorical_features = X.select_dtypes(include=['object']).columns

# Preprocess categorical and numerical features
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

# Apply preprocessing BEFORE oversampling
X_preprocessed = preprocessor.fit_transform(X)  # Preprocess the data

# Handle data imbalance using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_preprocessed, y)  # Use preprocessed data

# Split the resampled data
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)

# No need to preprocess again as it's already done
# X_train = preprocessor.fit_transform(X_train)
# X_test = preprocessor.transform(X_test)

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

###############################################

from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()
X_scaled = scaler.fit_transform(X)


##########################################

# visualize after handling imbalance

import matplotlib.pyplot as plt
# Visualize class distribution after SMOTE
plt.figure(figsize=(6, 4))
sns.countplot(x=y_resampled)
plt.title('Class Distribution After SMOTE')
plt.xlabel('Fraud Reported (0: No, 1: Yes)')
plt.ylabel('Count')
plt.show()

# apply data leakage handle

# Data leakage is a complex issue and requires careful analysis of the specific dataset and features.
# There's no generic code to handle all data leakage scenarios.

# Removing features that might leak information about the fraud AFTER it happened
potential_leakage_features = ['incident_severity', 'property_damage', 'bodily_injuries',
                               'witnesses', 'police_report_available', 'total_claim_amount',
                               'injury_claim', 'property_claim', 'vehicle_claim']

# Convert NumPy arrays back to DataFrames before using .drop()
X_train = pd.DataFrame(X_train)  # Convert X_train to DataFrame
X_test = pd.DataFrame(X_test)   # Convert X_test to DataFrame

X_train = X_train.drop(columns=potential_leakage_features, errors='ignore')
X_test = X_test.drop(columns=potential_leakage_features, errors='ignore')

# Train a Random Forest Classifier
rf = RandomForestClassifier(max_depth=None, min_samples_split=2, n_estimators=200)
[[186,  30],
 [ 31, 205]]
rf.fit(X_train, y_train)

# Predict on test set
y_pred = rf.predict(X_test)

# Evaluate the model
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Feature importance
feature_names = numerical_features.tolist() + list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features))
feature_importances = pd.Series(rf.feature_importances_, index=feature_names).sort_values(ascending=False)

# Plot feature importances
plt.figure(figsize=(10, 8))
sns.barplot(x=feature_importances, y=feature_importances.index)
plt.title('Feature Importances')
plt.show()

import pandas as pd

df = pd.read_csv("/content/multiTimeline (4).csv" , skiprows=1)

df

df.info()

df['Month'] = pd.to_datetime(df['Month']) # Assign the result back to the 'Month' column

df.set_index('Month', inplace=True)

df.plot(figsize=(15, 10))

import plotly.express as px
px.line(df, x=df.index, y=df.columns)

# prompt: if the data staionary

from statsmodels.tsa.stattools import adfuller

# Perform ADF test
result = adfuller(df['Category: All categories'])

# Print results
print('ADF Statistic:', result[0])
print('p-value:', result[1])
print('Critical Values:')
for key, value in result[4].items():
  print(f'   {key}: {value}')

# Interpret results
if result[1] <= 0.05:
  print('Data is stationary')
else:
  print('Data is non-stationary')